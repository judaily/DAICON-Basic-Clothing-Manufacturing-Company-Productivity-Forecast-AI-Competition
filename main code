import pandas as pd
import numpy as np
from sklearn.linear_model import BayesianRidge, HuberRegressor, Ridge, OrthogonalMatchingPursuit
from sklearn.ensemble import GradientBoostingRegressor
import lightgbm
from lightgbm import LGBMRegressor 
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('train.csv')
test = pd.read_csv('test.csv').drop(columns="ID")

categorial_features = ['quarter', 'department', 'day']
one_hot_encoder = {}
for categorial_feature in categorial_features:
    features = set(list(df[categorial_feature]))
    for fname in features:
        df[fname] = (df[categorial_feature]==fname)
        test[fname] = (test[categorial_feature]==fname)
    df = df.drop(columns=categorial_feature)
    test = test.drop(columns=categorial_feature)

X = df.drop(columns=["ID","actual_productivity"])
y = df["actual_productivity"]

br_params = {
    'n_iter': 304,
    'tol': 0.16864712769300896,
    'alpha_1': 5.589616542154059e-07,
    'alpha_2': 9.799343618469923,
    'lambda_1': 1.7735725582463822,
    'lambda_2': 3.616928181181732e-06
}

lightgbm_params = {
    'num_leaves': 5,
    'max_depth': 16,
    'learning_rate': 0.01,
    'n_estimators': 400
}

ridge_params = {
    'alpha': 631.1412445239156
}

models = {'gbr':GradientBoostingRegressor(),
          'br':BayesianRidge(**br_params),
          'ridge':Ridge(**ridge_params),
          'lgbm':LGBMRegressor(**lightgbm_params)}

for name, model in models.items():
  model.fit(X, y)

y_pred = (
    0.25 * models['gbr'].predict(test) +
    0.1 * models['br'].predict(test) +
    0.1 * models['ridge'].predict(test)+
    0.25 * models['lgbm'].predict(test))



sub_df = pd.read_csv('sample_submission.csv')
sub_df['actual_productivity'] = y_pred
sub_df.to_csv('first.csv',index= False)
